# Chatbot_using_Ollama

This project uses the Ollama service's Llama 3.2 model to run a chatbot through the streamlit server.

### Steps to setup :
1. pull llama3.2 to your local machine ( ollama pull llama3.2 )
2. install the modules required from the requirements.txt file ( pip install -r requirements.txt )
3. (optional) paste your langchain api key in the .env file to enable tracing of the project
4. now run ( streamlit run app.py )
